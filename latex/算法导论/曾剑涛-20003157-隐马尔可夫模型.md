# 隐马尔可夫模型





## 课程作业要求

本次作业是课程大作业。

请阅读以下整个文档，并补充完成整个文档。注意每一章节的问题部分，有对应分值，请完成。本次作业占课程分数 70%。

注意，本作业涉及到一定的概率论知识，要求对概率、条件概率、联合概率、贝叶斯公式、全概率公式较为熟悉。如果这方面数学知识不是很了解，虽不影响本作业的完成，但难度会较大，可考虑选择另一份作业“图的应用”完成。本作业提到了隐马尔可夫过程，但对马尔可夫过程的严格数学定义并不要求。

需要说明的是，本次是课程大作业，因此每一个问题请不要只写一个答案，**请尽量给出详细的解答过程**，避免因答案错误而一分不得。

完成文档后请输出为pdf文档，并按照如下方式重命名：**姓名-学号-隐马尔可夫模型.pdf**

## 概述

隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。本文档是对课本15章动态规划思考题15-7的扩展。参考阅读：1. 课程网站给出的英文文献；2. 李航《统计学系方法》。

隐马尔可夫模型主要是研究现实生活中这样的一类问题：一个事件，是由一系列状态决定，状态在每一个时刻$t$ 会以某种概率进行转换（这是马尔可夫模型）。然而，我们无法直接测量状态，只能观测到某状态下所生成的事件。这里状态到事件是第二个概率模型。这种状态无法观测，需要从状态产生的可观测事件来反推状态的，则是隐马尔可夫模型，即“状态转变的马尔可夫模型被隐藏在可观测事件”背后。举一个例子：假设笔记本的电池状态有“良好”、“老化”、“损毁”三种状态，则在人一个时刻$t$，电池必然处于一种状态，如“良好”，而在$t+1$时刻，电池会从“良好”状态以某种概率模型跳转到“良好”、“老化”、“损毁”中的一种状态。这是马尔可夫过程。然而我们很难直接检测电池的状态，我们直观上能看到的是笔记本的续航时间。比如，当电池处于“良好”状态的时候，有0.8的可能续航时间为10小时，0.2的可能续航时间为5小时，而在电池处于“老化”状态下，只有0.4的可能续航时间是10小时，0.6的可能续航时间为5小时。此时，我们根据观测到的续航时间的序列，以及两个概率（电池状态跳转概率与电池状态与续航时间之间的概率），则可以通过隐马尔可夫模型建模，从而求出电池的状态序列。

总结一下，我们来看看什么样的问题解决可以用HMM模型。使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。

有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。比如：我现在在打字写博客，我在键盘上敲出来的一系列字符就是观测序列，而我实际想写的一段话就是隐藏序列，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。再举一个，我在和你说话，我发出的一串连续的声音就是观测序列，而我实际要表达的一段话就是状态序列，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。以下是HMM的一些应用领域：

- 语音识别、中文分词或光学字符识别
- 机器翻译
- 生物信息学和基因组学
  - 基因组序列中蛋白质编码区域的预测
  - 对于相互关联的DNA或蛋白质族的建模
  - 从基本结构中预测第二结构元素
  - 通信中的译码过程
  - 地图匹配算法
- *还有更多...*

从这些例子中，我们可以发现，HMM模型可以无处不在。但是上面的描述还不精确，下面我们用精确的数学符号来表述我们的HMM模型。

## HMM模型的定义

对于HMM模型，首先我们假设$Q$是所有可能的隐藏状态的集合，𝑉V是所有可能的观测状态的集合，即：

$$Q={q_1,q_2,\ldots,q_N},\quad V={v_1,v_2,...v_M}$$

其中，𝑁是可能的隐藏状态数，𝑀是所有的可能的观察状态数。

对于一个长度为𝑇的序列，𝐼对应的状态序列, 𝑂是对应的观察序列，即：

$$𝐼={𝑖_1,𝑖_2,...,𝑖_𝑇}, \quad 𝑂={𝑜_1,𝑜_2,...𝑜_𝑇}$$

其中，任意一个隐藏状态$𝑖_t \in 𝑄$,任意一个观察状态$o_t\in V$。

HMM模型做了两个很重要的假设如下：

1. 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻$t$的隐藏状态是$i_t=q_i$，在时刻$t+1$的隐藏状态是$i_{t+1}=q_j$， 则从时刻$t$到时刻$t+1$的HMM状态转移概率$a_{ij}$可以表示为：

$$𝑎_{𝑖𝑗}=𝑃(𝑖_{𝑡+1}=𝑞_𝑗|𝑖_𝑡=𝑞_𝑖)$$

 　  这样$a_{ij}$可以组成马尔科夫链的状态转移矩阵$A$:

$$A=\left[aij \right]_{N×N}$$

2. 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻𝑡t的隐藏状态是$i_t=q_j$，而对应的观察状态为$o_t=v_k$，则该时刻观察状态$v_k$在隐藏状态$q_j$下生成的概率为$b_j(k)$，满足：

$$b_j(k)=P(o_t=v_k|i_t=q_j)$$

　　这样$b_j(k)$可以组成观测状态生成的概率矩阵$B$:

$B=[b_j(k)]_{N×M}$

　　除此之外，我们需要一组在时刻$t=1$的隐藏状态概率分布$\Pi$: 

$$\Pi=[\pi(𝑖)]_𝑁$$

​        其中$π(i)=P(i_1=q_i)$

一个HMM模型，可以由隐藏状态初始概率分布$\Pi$，状态转移概率矩阵$A$和观测状态概率矩阵$B$决定。$\Pi,A$决定状态序列，𝐵决定观测序列。因此，HMM模型可以由一个三元组$\lambda$表示如下：

$$𝜆=(𝐴,𝐵,\Pi)$$

###  一个HMM模型实例

下面我们用一个简单的实例来描述上面抽象出的HMM模型。

想象一个乡村诊所。村民有着非常理想化的特性，要么健康要么发烧。他们只有问诊所的医生的才能知道是否发烧。 聪明的医生通过询问病人的感觉诊断他们是否发烧。村民只回答他们感觉正常、头晕或冷。

假设一个病人每天来到诊所并告诉医生他的感觉。医生相信病人的健康状况如同一个离散马尔可夫链。病人的状态有两种“健康”和“发烧”，但医生不能直接观察到，这意味着这是隐藏状态。每天病人会告诉医生自己有以下几种由他的健康状态决定的感觉的一种：正常、冷或头晕。这些是观察结果。 整个系统为一个隐马尔可夫模型(HMM)。

医生知道村民的总体健康状况，还知道发烧和没发烧的病人通常会抱怨什么症状。 换句话说，医生知道隐马尔可夫模型的参数。你可以用程序语言（Python）写下来：

```python
states = ('Healthy', 'Fever')
 
observations = ('normal', 'cold', 'dizzy')
 
start_probability = {'Healthy': 0.6, 'Fever': 0.4}
 
transition_probability = {
   'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},
   'Fever' : {'Healthy': 0.4, 'Fever': 0.6},
   }
 
emission_probability = {
   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},
   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},
   }
```

在这段代码中, 起始概率`start_probability` $\Pi$ 表示病人第一次到访时医生认为其所处的HMM状态，他唯一知道的是病人倾向于是健康的。 转移概率`transition_probability` $A$表示潜在的马尔可夫链中健康状态的变化。在这个例子中，当天健康的病人仅有30%的机会第二天会发烧。放射概率`emission_probability` $B$表示每天病人感觉的可能性。假如他是健康的，50%会感觉正常。如果他发烧了，有60%的可能感觉到头晕。

描述这个过程的HMM可用下图表示：

![Graphical representation of the given HMM](/Users/admin/SynologyDrive/ECUST/教学/算法导论/大作业/An_example_of_HMM.png)

### HMM 观测序列的生成（问题1）

根据上述的描述，请给出HMM观测序列生成过程的伪代码。

Input: HMM的模型$λ=(A,B,Π)$, 观测序列的长度$T$

Output: 观测序列$𝑂={𝑜_1,𝑜_2,...𝑜_𝑇}$

Step 1: 根据初始状态概率分布$\Pi$生成隐藏状态$i_1$.

Step 2: for t from 1 to T

(补充完成Step 2需要的内容，注意，包括隐藏状态的生成，以及根据隐藏状态生成观测状态)

Final: 所有的$o_t$一起形成观测序列$𝑂={𝑜_1,𝑜_2,...𝑜_𝑇}$

__Mark: 10 points__

### HMM模型三个基本问题

1. 计算观察序列概率，称之为似然（Likelihood）问题。即给定模型$λ=(A,B,Π)$和观测序列$O={o_1,o_2,...o_T}$，计算在模型$λ$下观测序列$O$出现的概率$P(O|λ)$。这个问题的求解需要用到**前向算法**(Forward Algorithm)，是HMM模型三个问题中最简单的。
2. 预测问题，也称为解码问题。即给定模型$λ=(A,B,Π)$和观测序列$O={o_1,o_2,...o_T}$，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的**维特比算法**(Viterbi algorithm)。这个问题是HMM模型三个问题中复杂度居中的算法。
3. 模型参数学习问题。即给定观测序列$O={o_1,o_2,...o_T}$，估计模型$λ=(A,B,Π)$的参数，使该模型下观测序列的条件概率$P(O|λ)$最大。这个问题的求解需要用到基于最大期望算法（Expectation Maximization，EM算法）的**鲍姆-韦尔奇算法**（Baum-Welch算法），也被称之为前向后向算法。这个问题是HMM模型三个问题中最复杂的，在本文不做介绍。

### 前向算法（问题2、问题3，共40 points）

我们首先求解第一个问题，我们已知HMM模型的参数$λ=(A,B,Π)$。其中$A$是隐藏状态转移概率的矩阵，$𝐵$是观测状态生成概率的矩阵， $Π$是隐藏状态的初始概率分布。同时我们也已经得到了观测序列$O={o_1,o_2,...o_T}$，现在我们要求观测序列$O$在模型$λ$下出现的条件概率$P(O|λ)$。

这个问题看上去非常简单，因为我们知道所有的隐藏状态之间的转移概率和所有从隐藏状态到观测状态生成概率，那么我们穷举即可。以下是该方法的描述：

任意一个隐藏序列$I={i_1,i_2,...,i_T}$出现的概率是：

$$P(I|λ)=π_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_{T−1}i_T}$$

对于固定的状态序列$I={i_1,i_2,...,i_T}$，我们要求的观察序列$O={o_1,o_2,...o_T}$出现的概率是：

$$P(O|I,λ)=b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)$$

则$O$和$I$联合出现的概率是：

$P(O,I|λ)=P(I|λ)P(O|I,λ)=π_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_{T−1}i_T}b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)$

然后求边缘概率分布，即可得到观测序列$O$在模型$λ$下出现的条件概率$P(O|λ)$：

$$𝑃(𝑂|𝜆)=∑_𝐼𝑃(𝑂,𝐼|𝜆)=∑_Iπ_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_{T−1}i_T}b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)$$

在这里，$I$代表所有隐藏序列的可能性，即所有隐藏序列的排列组合。

### 问题2

上述算法的计算复杂度是非常高的。若假设每一次四则运算需要消耗单位时间1，$N$代表隐藏状态的个数，求上述算法的计算复杂度。

**Mark: 10 points**

### 问题3（Mark 30 points)

实际上在上述求解的过程，我们看到了一个重要事实，即*隐马尔可夫模型是一个典型的状态推移过程，整个问题的解由子问题构成，具备最优子结构，且后续子问题的求解依赖于前序子问题*。这正是典型的**动态规划**问题。请复习动态规划一章，并注意对比隐马尔可夫模型和斐波那契数列推导的相似性。接下来将使用动态规划算法重构上述过程，并求解其计算复杂度。

和动态规划问题类似，我们按照动态规划的五步法来来构建：

1. 定义子问题。我们定义时刻$t$时隐藏状态为$q_i$，观测状态的序列为$𝑜_1,𝑜_2,...𝑜_𝑡$的概率为子问题，又称之为前向概率，$f_t(i) = P(o_1,o_2,...o_t,i_t=q_i|λ)$。
2. 猜测。假设$q_i$已经得到，则 t->t+1时刻，实际上$f_{t+1}(i)$的状态，是由$N$个隐藏状态$q_1,...q_N$，跳转到$q_i$，并且此时从隐藏状态$q_i$ -> 观测状态$o_{t+1}$。
3. （**Mark: 10 points**) 子问题求解。请根据2，写出$f_{t+1}(i)$与$f_t(i)$之间的递推关系式，并依据此证明该问题具有最优子结构，可适用于动态规划算法。
4. 递归+求解。根据2、3获得了递归过程，这里需要构建TxN矩阵所有$f_t(i)$。这里的初始状态是$f_1(𝑖)=𝜋_𝑖𝑏_𝑖(𝑜_1),𝑖=1,2,...𝑁$

5. (**Mark: 10 points** ）根据1-4完成原问题的解法，通过结合所有子问题的解$f_T(i)$，求出原问题。补充完成前向算法的伪代码。

Input：HMM模型$λ=(A,B,Π)$，观测序列$𝑂=(𝑜_1,𝑜_2,...𝑜_𝑇)$

Output：观测序列概率$𝑃(𝑂|𝜆)$

for state i=1 to N

​	Initialize: 计算1时刻隐藏状态$i$的前向概率

​	(**补充初始状态计算前向概率的操作**)

​	Recursion: 递推时刻$2,3,...T$时刻的前向概率

​	for t = \____ to \_\_\_\_\_:

​		for $i=$ \_\_\_\_\_\_ to \_\_\_\_\_\_:

​				$f_{t+1}(i) = .....$

(**补充t状态计算前向概率的操作以及for循环的次数**)

Final: $P(O|\lambda) = .....$

（**Mark: 10 points**）根据上述过程，给出前向算法的算法复杂度。

## Viterbi算法（问题4，共30 points）

接下来，我们研究HMM的一个重要问题，即给定模型和观测序列，求给定观测序列条件下，最可能出现的对应的隐藏状态序列。这是HMM的解码问题，也是该模型最为广泛应用的一种场景。HMM模型的解码问题最常用的算法是维特比算法，当然也有其他的算法可以求解这个问题。同时维特比算法是一个通用的求序列最短路径的动态规划算法，也可以用于很多其他问题。我们采用通用的动态规划算法来构造这个问题的解，并回答问题四。

在HMM模型的解码问题中，给定模型$λ=(A,B,Π)$和观测序列$𝑂={𝑜_1,𝑜_2,...𝑜_𝑇}$，求给定观测序列O条件下，最可能出现的对应的状态序列$𝐼^∗={𝑖^∗_1,𝑖^∗_2,...𝑖^∗_𝑇}$，即$𝑃(𝐼^∗|𝑂)$要最大化。实际上，即是在给定$𝑂={𝑜_1,𝑜_2,...𝑜_𝑇}$，我们穷举所有可能的$𝐼={𝑖_1,𝑖_2,...𝑖_𝑇}$序列，并计算$P(I|O)$，哪一个值最大则选取哪一组$I$为$I^*$。接下来我们遵循动态规划五步法来构建算法：

1. 子问题的定义。我们定义，时刻$𝑡$隐藏状态为$i$ 所有可能的状态转移路径$𝑖_1,𝑖_2,...𝑖_𝑡$中的概率最大值为$v_t(i)$：

$$v_t(i) = \max 𝑃(𝑖_𝑡=𝑖,𝑖_1,𝑖_2,...𝑖_{𝑡−1},𝑜_𝑡,𝑜_{𝑡−1},...𝑜_1|𝜆),𝑖=1,2,...𝑁$$

这个概率是在$\lambda$模型下，观察为$o_1,…,o_t$，状态为$q_0,…,q_{t−1},i$的**联合概率**。$v_t(i)$它要求在所有长度为$t$的路径中寻找最大的联合概率(当然也需要找到这条路径)，要求是$t$时刻的状态是一定为$i$。

2. 猜测。假设$v_t(i)$已经得到，则 $t \rightarrow t+1$ 时刻，$v_{t+1}(i)$ 代表$t+1$时刻，观测序列为$o_1,…,o_{t+1}$，且此时状态为$i$的最大联合概率。很显然，我们需要先求$N$个隐藏状态$q_1,...q_N$，跳转到$q_i$，并且此时从隐藏状态$q_i$ 到观测状态$o_{t+1}$的概率，而$v_{t+1}(i)$是这一系列概率中的最大值。注意这里和前向算法的不同，前向算法是求和，这里是求最大值。

3. （**Mark: 10 points**) 子问题求解。请根据2，写出$v_{t+1}(i)$与$v_t(i)$之间的递推关系式，并依据此证明该问题具有最优子结构，可适用于动态规划算法。

4. 递归+求解。根据2、3获得了递归过程，这里需要构建$T*N$矩阵或是相应的数据结构存储所有$v_t(i)$。这里的初始状态是$v_1(𝑖)=𝜋_𝑖𝑏_𝑖(𝑜_1),𝑖=1,2,...𝑁$。注意到一个问题，从3的递归式子，我们只记录了$t$时刻的隐藏状态$i$出现的联合概率$v_t(i)$，并没有记录具体的状态。根据定义，时刻$t$隐藏状态为$i$，$\Phi_t(i)$代表$t-1$时刻所处的状态，则$\Phi_t(i)$是使得转移概率最大的隐藏状态，即：

   $$ \Phi_t(i) = \arg \max_{1\leq j \leq N} v_{t-1} a_{ji} $$

5. (**Mark: 20 points** ）根据1-4的讨论，我们知道T时刻，最终的$𝑃(𝐼^∗|𝑂)$为：

$$ 𝑃(𝐼^∗|𝑂) = \max_{1\leq i \leq N} v_T(i) $$

此时，有

$$ i^*_T = \arg \max_{1\leq i \leq N} v_T(i) $$，

即使得上述$𝑃(𝐼^∗|𝑂)$成立的$i$。此时我们有$\Phi_T(i)=i^*_T$。因此，为了求得隐藏状态序列，我们从$t=1$开始构建完$v_t(i)$矩阵，然后从$t=T-1$时刻开始，逆向求每一个隐藏状态：

$$ i^*_t = \Phi_{t+1}(i^*_{t+1})$$

**根据上述讨论，请总结为伪代码（或是某种编程语言完成）（10 points)，并简要分析一下算法复杂度（10 points）**。

## HMM的应用 -- 中文分词（Mark: 20 points）

HMM模型在语音识别与自然语言处理中有广泛应用。接下来，我们以中文分词为例子，讲述一下HMM的应用。

中文分词是自然语言处理中一个最基础的问题，就是一段话怎么切分。比如：

`今天的天气怎么样`

可以按如下方式分词

`今天\天气\怎么样`

这样一个任务交给机器自动切分，很容易想到基于词典的匹配，但是匹配会面临如下问题：

1. 词典需要人工不停录入，否则没法识别新词。这也就是机器学习中一直强调的泛化能力；

2. 不同场景下分词方式不一样，比如：

   `南京\市长\是\个\好\同志`

而下面这句话就不能把“市长”分到一起了：

`南京市\长江\大桥`

因此，我们需要构建更先进的分词方法，其中一种则是基于统计的方法。首先我们转换下思维，把分词问题做个转换：分词问题就是对句子中的每个字打标注，标注要么是一个词的开始（B），要么是一个词的中间位置（M），要么是一个词的结束位置（E），还有单个字的词，用S表示。比如下面的句子可以这样标注：

`我想去乌鲁木齐`

`SSSBMME`
结合上述讨论：

1. （**10 points**) 如果采用HMM模型来做，请问这里的状态、观测序列、状态转移矩阵、观测状态概率矩阵分别对应什么呢？
2. (**10 points**) 请查阅相关资料，假设给定了分好词的语料库，请问：a) 初始状态的概率分布是什么，如何求？ b) 如何统计并得到状态转移矩阵和观测状态概率矩阵？ c) 训练好后，给定一个句子，应该使用什么算法以及如何来做分词？

## 致谢

本作业到此就结束了。感谢大家这个学期的选课和支持！

